import google.generativeai as genai
import json
import sqlite3
from datetime import datetime, timedelta
import logging

import asyncio
import glob
import os
from pathlib import Path



EVALUATION_INSTRUCTION = """ 
Here is the `question`, `model_response`, and `ground_truth` solution that you need to evaluate:

<question>
{question}
<question>

<model_response>
{model_response}
</model_response>

<ground_truth>
{ground_truth}
</ground_truth>

Given a numerical finance question, model_response (generated by a large language model (LLM)), and a ground_truth solution, 
you need to evaluate the model_response on 7 criterias. 
Assess the model_response by answering all the questions under each criteria.

## Evaluation Criteria:
**1. Planning and Reasoning Flow**
a. Does the solution demonstrate a clear plan before performing calculations? - Possible answers: Yes; No; Unable to determine
b. Are the steps logically sequenced, leading naturally to the final answer? - Possible answers: Yes; No; Unable to determine
c. Is the reasoning coherent and connected? - Possible answers: Yes; No; Unable to determine

**2. Mathematical Correctness**
a. Are all mathematical calculations accurate? - Possible answers: Yes; No; Unable to determine
b. Are all assumptions clearly stated and mathematically valid? - Possible answers: Yes; No; Unable to determine
c. Are units (e.g., %, $, years) used consistently and correctly? - Possible answers: Yes; No; Unable to determine
d. Are rounding of floating point values correctly handled? - Possible answers: Yes; No; Unable to determine

**3. Formula Usage and Application**
a. Are the correct financial formulas used at each step (with respect to ground_truth)? - Possible answers: Yes; No; Unable to determine
b. Are the formulas applied and simplified correctly (with respect to ground_truth)? - Possible answers: Yes; No; Unable to determine

**4. Simplicity and Efficiency**
a. Is the solution free of unnecessary or repetitive steps? - Possible answers: Yes; No; Unable to determine
b. Is the reasoning direct and concise without sacrificing completeness? - Possible answers: Yes; No; Unable to determine
c. Could the same result be obtained more efficiently? - Possible answers: Yes; No; Unable to determine

**5. Financial Contextual Understanding**
a. Does the model interpret and apply financial terminologies correctly (e.g., "cost of capital", "discount factor", "leverage")? - Possible answers: Yes; No; Unable to determine
b. Is domain-specific knowledge used meaningfully? - Possible answers: Yes; No; Unable to determine
c. Are concepts aligned with real-world financial practice? - Possible answers: Yes; No; Unable to determine

**6. Formatting and Presentation**
a. Are values presented using financial formatting standards (currency, commas, decimal precision, percentages)? - Possible answers: Yes; No; Unable to determine
b. Are steps, variables, or tables clearly labeled? - Possible answers: Yes; No; Unable to determine
c. Is the final answer clearly indicated? - Possible answers: Yes; No; Unable to determine

**7. Correctness of Final Answer**
a. Extract the final_answer from model_response and ground_truth, then test for equality. Are they equal? - Possible answers: Yes; No; Unable to determine
   

## Instructions:
1. **Read** the question, model_truth, and ground_truth solution carefully.
2. **Sequentially evaluate** each question (1a, 1b, 1c, 2a, 2b, 2c, 2d, 3a, 3b, 4a, 4b, 4c, 5a, 5b, 5c, 6a, 6b, 6c, 7a), focusing ONLY on one question at a time indicated by the question_number (ignoring all other questions).
3. For each question, **provide detailed justification**, quoting relevant segments from question, model_response, or ground_truth solution.
4. **For questions 1a to 7a, select the single best answer** from the provided options.
7. **Before your final evaluation**, wrap your reasoning in `"evaluation_process"`, showing how you arrived at each judgement.
8. Use **"Unable to determine"** if insufficient information is present.

Please provide your evaluation in the following JSON format (ensuring that it is parsable by json.loads()):

{{
  "evaluation_process": "[Your detailed thought process here]",
  "answers": [
    {{
      "question_number": "1a",
      "analysis": "[Your analysis]",
      "justification": "[Why you chose that answer]",
      "answer": "[Selected answer]"
    }},
    ...,
    {{
      "question_number": "7a",
      "analysis": "[Your analysis]",
      "justification": "[Why you chose that answer]",
      "answer": "[Selected answer]"
    }}
  ]
}}

Please provide your evaluation in the above mentioned JSON format (ensuring that it is parsable by json.loads()):"""

# Configure logging
def setup_logging():
    log_filename = f"llm-as-a-judge-response-gen_log.log"

    os.makedirs(f"logs", exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[
            logging.FileHandler(f"logs/{log_filename}"),
            logging.StreamHandler()
        ]
    )

class RateLimiter:
    def __init__(self, db_path='rate_limits.db'):
        self.db_path = db_path
        self.setup_database()
        
    def setup_database(self):
        with sqlite3.connect(self.db_path) as conn:
            # Create table for API requests with token count
            conn.execute('''
                CREATE TABLE IF NOT EXISTS api_requests (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME,
                    request_type TEXT,
                    token_count INTEGER
                )
            ''')
            
            # Create table for usage metrics
            conn.execute('''
                CREATE TABLE IF NOT EXISTS usage_metrics (
                    timestamp DATETIME,
                    rpm INTEGER,
                    rpd INTEGER,
                    tpm INTEGER
                )
            ''')
    
    def get_current_metrics(self):
        current_time = datetime.now()
        minute_ago = current_time - timedelta(minutes=1)
        day_ago = current_time - timedelta(days=1)
        
        with sqlite3.connect(self.db_path) as conn:
            # Get RPM
            rpm = conn.execute('''
                SELECT COUNT(*) FROM api_requests 
                WHERE timestamp > ?
            ''', (minute_ago,)).fetchone()[0]
            
            # Get RPD
            rpd = conn.execute('''
                SELECT COUNT(*) FROM api_requests 
                WHERE timestamp > ?
            ''', (day_ago,)).fetchone()[0]
            
            # Get TPM (Tokens Per Minute)
            tpm = conn.execute('''
                SELECT COALESCE(SUM(token_count), 0) FROM api_requests 
                WHERE timestamp > ?
            ''', (minute_ago,)).fetchone()[0]
            
        return rpm, rpd, tpm
    
    def can_make_request(self):
        rpm, rpd, tpm = self.get_current_metrics()
        return rpm < 10 and rpd < 500 and tpm < 250000
    
    def log_request(self, token_count):
        current_time = datetime.now()
        
        with sqlite3.connect(self.db_path) as conn:
            # Log the request
            conn.execute('''
                INSERT INTO api_requests (timestamp, request_type, token_count)
                VALUES (?, ?, ?)
            ''', (current_time, 'solution_evaluation', token_count))
            
            # Update metrics
            rpm, rpd, tpm = self.get_current_metrics()
            conn.execute('''
                INSERT INTO usage_metrics (timestamp, rpm, rpd, tpm)
                VALUES (?, ?, ?, ?)
            ''', (current_time, rpm, rpd, tpm))

    def get_usage_stats(self):
        with sqlite3.connect(self.db_path) as conn:
            return conn.execute('''
                SELECT timestamp, rpm, rpd, tpm 
                FROM usage_metrics 
                ORDER BY timestamp DESC 
                LIMIT 1
            ''').fetchone()

class SolutionEvaluator:
    def __init__(self, api_key, api_key_idx, model_name):
        self.rate_limiter = RateLimiter(db_path=f"rate_limits_{api_key_idx}.db")
        genai.configure(api_key=api_key)
        self.api_key_idx = api_key_idx
        self.model_name = model_name
        self.model = genai.GenerativeModel(model_name)
        
    async def get_gemini_response(self, prompt: str):
        logger = logging.getLogger(__name__)
        """Make a Gemini API call and track usage"""
        input_tokens = self.model.count_tokens(prompt).total_tokens
        try:
            response = await self.model.generate_content_async(prompt, 
                                                               generation_config={
                                                                    "temperature": 0.5,
                                                                    "top_p": 0.9,
                                                                    "max_output_tokens": 16000,  # Maximum length of the response
                                                               },    
                                                            )
            
            
            try:
                if hasattr(response, 'text'):
                    response_text = response.text
                else:
                    # Handle the AsyncGenerateContentResponse
                    response_text = response.result.text
            except:
                # logger.info(f"Feedback: {response.prompt_feedback}")
                if not response.candidates:
                    response_text = str(response.prompt_feedback)

            try:
                response_text = getattr(response, "text", None) or getattr(response.result, "text", None)
                output_tokens = self.model.count_tokens(response_text).total_tokens
                if not response_text:
                    raise ValueError("Empty response text.")
            except Exception as e:
                logger.error(f"Failed to extract response text: {e}")
                response_text = ""
                output_tokens = 0

            return {
                "response": response_text, 
                "input_tokens": input_tokens, 
                "output_tokens": output_tokens
            }
        
        except Exception as e:
            logger.error(f"Error in get_gemini_response: {str(e)}")
            return None
        
    async def safe_get_response(self, prompt, retries=3):
        for attempt in range(retries + 1):
            rpm, rpd, tpm = self.rate_limiter.get_current_metrics()
            if rpd >= 500:
                self.api_key_idx += 1
                self.rate_limiter = RateLimiter(db_path=f"rate_limits_{self.api_key_idx}.db")
                genai.configure(api_key=os.getenv(f"GOOGLE_API_KEY_{self.api_key_idx}"))
                self.model = genai.GenerativeModel(self.model_name)
            response = await self.get_gemini_response(prompt)
            
            if response!=None and response["response"]!="":
                return response
            await asyncio.sleep(10)
        return None

    async def evaluate_solution(self, question, model_response, ground_truth):
        logger = logging.getLogger(__name__)
        if not self.rate_limiter.can_make_request():
            wait_time = 60
            logger.warning(f"Rate limit reached. Waiting {wait_time} seconds...")
            await asyncio.sleep(wait_time)
                
        try:
            # Make the first API request
            prompt = EVALUATION_INSTRUCTION.format(
                question=question,
                model_response=model_response,
                ground_truth=ground_truth,
            )

            # print(prompt)
            
            output = await self.safe_get_response(prompt)
            if output is None:
                return None
                
            rpm, rpd, tpm = self.rate_limiter.get_current_metrics()
            estimated_tokens = output["input_tokens"] + output["output_tokens"]
            self.rate_limiter.log_request(estimated_tokens)
                
            # Log current usage metrics
            logger.info(f"Current Usage - RPM: {rpm}, RPD: {rpd}, TPM: {tpm}")
            
            now = datetime.now()
            timestamp = now.strftime("%d-%m-%Y_%H-%M-%S")
            return {"prompt": prompt, "response": output["response"], 
                    "token_usage_stats": {"rpm": rpm, "rpd": rpd, "tpm": tpm},
                    "api_key_index": self.api_key_idx,
                    "model_name": self.model_name,
                    "date_time": timestamp}
                
        except Exception as e:
            logger.error(f"Error during evaluation: {str(e)}")
            return None

def print_usage_stats(stats):
    if stats:
        timestamp, rpm, rpd, tpm = stats
        print("\nCurrent Usage Statistics:")
        print(f"Timestamp: {timestamp}")
        print(f"Requests per minute (RPM): {rpm}")
        print(f"Requests per day (RPD): {rpd}")
        print(f"Tokens per minute (TPM): {tpm}")
        
        # Print warnings if approaching limits
        if rpm >= 8:  # 80% of RPM limit
            print("WARNING: Approaching RPM limit!")
        if rpd >= 400:  # 80% of RPD limit
            print("WARNING: Approaching RPD limit!")
        if tpm >= 200000:  # 80% of TPM limit
            print("WARNING: Approaching TPM limit!")

async def main():
    setup_logging()
    logger = logging.getLogger(__name__)

    llm_judge_model_name = 'gemini-2.5-flash-preview-04-17'
    results_dir = "results"
    output_dir = "metrics/llm-as-a-judge/responses"
    os.makedirs(output_dir, exist_ok=True)
    batch_size = 3
    chosen_models_list = ["finance_LLM_13B", "deepseek_r1_distill_llama_70b", "llama3p3_70b_instruct", "fino1_8b", "fin_r1"]
    chosen_model_result_filepath_list = [fpath for fpath in glob.glob(f"{results_dir}/*.jsonl") if any(keyword in fpath for keyword in chosen_models_list)]

    idx = 1
    for model_result_fpath in chosen_model_result_filepath_list:
        model_name = Path(model_result_fpath).stem
        print(model_name)
        with open(model_result_fpath, "r") as rf:
            content = [json.loads(row) for row in rf.readlines()]
            # print(content[0].keys())
            now = datetime.now()
            timestamp = now.strftime("%d-%m-%Y_%H-%M-%S")
            with open(f"{output_dir}/{model_name}_{timestamp}.jsonl", "w", encoding="utf-8") as wf:
                evaluator = SolutionEvaluator(os.getenv(f"GOOGLE_API_KEY_{idx}"), idx, llm_judge_model_name)
                
                logger.info(f"Loading Responses of {model_name}")
                payloads = [{"question": row["question"], 
                            "model_response": row["generation"], 
                            "ground_truth": row["solution"]} for row in content]
                
                results_count = 0
                for start in range(0, len(payloads), batch_size):
                    end = start + batch_size
                    batch_index = start // batch_size + 1
                    if end >= len(payloads):
                        batch_ori = content[start:]
                        batch = payloads[start:]
                    else:
                        batch_ori = content[start:end]
                        batch = payloads[start:end]
                    
                    logger.info(f"Processing batch {batch_index}: prompts {start + 1} to {min(end, len(payloads))}")
                    
                    results = await asyncio.gather(
                        *(evaluator.evaluate_solution(
                        question=item["question"],
                        model_response=item["model_response"],
                        ground_truth=item["ground_truth"]) for item in batch)
                    )
                    results_count += len(results)
                    for row, result in zip(batch_ori, results):
                        row["llm-as-a-judge_response"] = result
                        wf.write(json.dumps(row, ensure_ascii=False) + "\n")
                        wf.flush()
                    logger.info(f"Batch {batch_index} completed, collected {results_count}")
                    await asyncio.sleep(1)

                                        
if __name__ == "__main__":
    asyncio.run(main())
